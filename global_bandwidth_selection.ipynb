{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c72d16c7",
   "metadata": {},
   "source": [
    "# Practical Exercice: Global bandwidth selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a69926c",
   "metadata": {},
   "source": [
    "The aim of this practical exercise is to estimate the optimal global bandwidth  for local linear regression using the Asymptotic Mean Integrated Squared Error (AMISE) criterion. \n",
    "The theoretical expression of the AMISE, optimal bandwidth under homoscedasticity and a quartic (biweight) kernel is given by\n",
    "\n",
    "$$\n",
    "h_{\\mathrm{AMISE}} \\;=\\; n^{-1/5}\\left( \\frac{35 \\, \\sigma^2 \\, \\lvert \\mathrm{supp}(X)\\rvert}{\\theta_{22}} \\right)^{1/5} $$\n",
    "\n",
    "where $\\sigma^2 = \\mathrm{Var}(\\varepsilon)$ and $\\theta_{22} \\;=\\; \\int \\big(m''(x)\\big)^2 f_X(x)\\, dx$.\n",
    "\n",
    "Since both $\\sigma^2$ and $\\theta_{22}$ are unknown, they must be estimated from the data. Following the course notes, the strategy is to split the sample into $N$ blocks and, in each block, fit a quartic regression model of the form \n",
    "$$y_i = \\beta_{0j} + \\beta_{1j} x_i + \\beta_{2j} x_i^2 + \\beta_{3j} x_i^3 + \\beta_{4j} x_i^4 + \\varepsilon_i\n",
    "$$ \n",
    "where $x_i$ and $y_i$ are the observations belonging to block $j$.\n",
    "\n",
    "From these blockwise polynomial fits, we obtain estimates of the regression function $\\hat m_j(x)$ and  its second derivative $\\hat m_j''(x)$. These are then used to form the plug-in estimators\n",
    "\n",
    "- $\\widehat{\\theta}_{22}(N) = \\frac{1}{n} \\sum_{i=1}^n \\sum_{j=1}^N \n",
    "\\Big( \\hat m_j''(X_i) \\Big)^2 \\,\\mathbf{1}_{\\{X_i \\in \\mathcal{X}_j\\}}$\n",
    "\n",
    "- $\\hat\\sigma^2(N) = \\frac{1}{n - 5N} \\sum_{i=1}^n \\sum_{j=1}^N \n",
    "\\big( Y_i - \\hat m_j(X_i) \\big)^2 \\,\\mathbf{1}_{\\{X_i \\in \\mathcal{X}_j\\}}.\n",
    "$\n",
    "\n",
    "\n",
    "These plug-in estimates are then substituted into the formula of $h_{\\mathrm{AMISE}}$  to yield a data-driven bandwidth choice. \n",
    "\n",
    "In this simulation study, we generate data with \n",
    "- covariates $X \\sim \\mathrm{Beta}(\\alpha,\\beta)$ and \n",
    "- responses  $Y = m(X) + \\varepsilon$, where $m(x) = \\sin\\!\\Big( \\big(\\tfrac{x}{3} + 0.1\\big)^{-1} \\Big), \\varepsilon \\sim \\mathcal{N}(0,\\sigma^2)$.\n",
    "\n",
    "We fix $\\sigma^2 = 1$ and explore how the estimated bandwidth $\\hat h_{\\mathrm{AMISE}}$ behaves \n",
    "as a function of the sample size $n$, the block size $N$, and the Beta distribution parameters $(\\alpha,\\beta)$. \n",
    "The results are summarised in the figure **global\\_bandwidth\\_selection.png**, \n",
    "which provides a visual assessment of the influence of these parameters on the global bandwidth choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b01b02c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Importing librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6eeb9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09865bec",
   "metadata": {},
   "source": [
    "## Model & simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e5b7ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def m_true(\n",
    "        x: np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Regression function m(x) = sin( (x/3 + 0.1)^(-1) ).\n",
    "\n",
    "    Parameters\n",
    "        - x: Points in [0, 1]\n",
    "\n",
    "    Returns\n",
    "        - m(x) at each entry of x (np.ndarray)\n",
    "        \n",
    "    \"\"\"\n",
    "    return np.sin(1.0 / (x / 3.0 + 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46f210e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(\n",
    "        n: int, \n",
    "        alpha: float, \n",
    "        beta: float, \n",
    "        sigma: float, \n",
    "        rng: np.random.Generator\n",
    "    ) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Simulate (X, Y) with X ~ Beta(alpha, beta), Y = m(X) + eps, eps ~ N(0, sigma^2)\n",
    "    \"\"\"\n",
    "    X = rng.beta(alpha, beta, size=n)\n",
    "    eps = rng.normal(0.0, sigma, size=n)\n",
    "    Y = m_true(X) + eps\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7564503",
   "metadata": {},
   "source": [
    "##  Blocking & OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77918be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_blocks(\n",
    "        X: np.ndarray, \n",
    "        N: int\n",
    "    ) -> list[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Partition indices {0,...,n-1} into N contiguous blocks in ascending order of X\n",
    "\n",
    "    Parameters\n",
    "        - X: Covariate values\n",
    "        - N: Number of blocks \n",
    "\n",
    "    Returns\n",
    "        - Each array contains the indices belonging to block j (list of np.ndarray)        \n",
    "    \"\"\"\n",
    "    n = X.size\n",
    "    order = np.argsort(X)\n",
    "    splits = np.array_split(order, N)\n",
    "    return [np.sort(b) for b in splits]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ff3e461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def design_matrix_quartic(\n",
    "        x: np.ndarray\n",
    "    ) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Build [1, x, x^2, x^3, x^4]\n",
    "\n",
    "    Returns\n",
    "        - np.ndarray of shape (len(x), 5)\n",
    "    \"\"\"\n",
    "    return np.column_stack([np.ones_like(x), x, x**2, x**3, x**4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a3d0c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_fit_and_eval(\n",
    "        block_x: np.ndarray, \n",
    "        block_y: np.ndarray, \n",
    "        eval_x: np.ndarray\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Fit quartic OLS y ~ [1, x, x^2, x^3, x^4] on (block_x, block_y),\n",
    "    evaluate m_hat and m_hat'' at eval_x\n",
    "\n",
    "    Returns\n",
    "        - m_hat_eval: Fitted function (np.ndarray)\n",
    "        - m2_hat_eval: Second derivative at eval_x (np.ndarray)\n",
    "    \"\"\"\n",
    "    Xd = design_matrix_quartic(block_x)\n",
    "    \n",
    "    # OLS via least squares\n",
    "    beta, *_ = np.linalg.lstsq(Xd, block_y, rcond=None)\n",
    "    \n",
    "    # Evaluate fitted polynomial and its second derivative\n",
    "    e = design_matrix_quartic(eval_x) @ beta\n",
    "    \n",
    "    # For p(x) = b0 + b1 x + b2 x^2 + b3 x^3 + b4 x^4 :\n",
    "    # p''(x) = 2 b2 + 6 b3 x + 12 b4 x^2\n",
    "    m2 = 2.0 * beta[2] + 6.0 * beta[3] * eval_x + 12.0 * beta[4] * (eval_x**2)\n",
    "    return e, m2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589dc6b2",
   "metadata": {},
   "source": [
    "## Plug-in estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "328097b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def theta22_hat(\n",
    "        X: np.ndarray, \n",
    "        Y: np.ndarray, \n",
    "        N: int\n",
    "    ) -> float:\n",
    "    \"\"\"\n",
    "    Estimate theta22_hat\n",
    "\n",
    "    Parameters\n",
    "        - X, Y: Data \n",
    "        - N : Number of blockes\n",
    "\n",
    "    Returns\n",
    "        - Estimate of theta_22 (float)\n",
    "    \"\"\"\n",
    "    blocks = make_blocks(X, N)\n",
    "    n = X.size\n",
    "    acc = 0.0\n",
    "    for idx in blocks:\n",
    "        xb, yb = X[idx], Y[idx]\n",
    "        m_hat, m2_hat = poly_fit_and_eval(xb, yb, xb)\n",
    "        acc += np.sum(m2_hat * m2_hat)\n",
    "    return acc / n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54405c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma2_hat(\n",
    "        X: np.ndarray, \n",
    "        Y: np.ndarray, \n",
    "        N: int\n",
    "    ) -> float:\n",
    "    \"\"\"\n",
    "    Estimate of sigma^2\n",
    "    The denominator uses (n - number_of_parameters_per_block * N) = (n - 5N), since each quartic regression fits 5 coefficients\n",
    "\n",
    "    Returns\n",
    "        - Estimate of sigma^2 (float)\n",
    "    \"\"\"\n",
    "    blocks = make_blocks(X, N)\n",
    "    n = X.size\n",
    "    rss = 0.0\n",
    "\n",
    "    for idx in blocks:\n",
    "        xb, yb = X[idx], Y[idx]\n",
    "        m_hat, _ = poly_fit_and_eval(xb, yb, xb)\n",
    "        rss += np.sum((yb - m_hat)**2)\n",
    "    denom = n - 5 * N\n",
    "\n",
    "    if denom <= 0: # fall back to simple (n) to avoid division by zero\n",
    "        denom = n\n",
    "\n",
    "    return rss / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6bfed95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def h_AMISE_hat(\n",
    "        n: int, \n",
    "        sigma2: float, \n",
    "        theta22: float, \n",
    "        support_len: float = 1.0\n",
    "    ) -> float:\n",
    "    \"\"\"\n",
    "    Plug into h_AMISE\n",
    "    For Beta(a,b), |supp(X)| = 1.\n",
    "    \"\"\"\n",
    "    val = (35.0 * sigma2 * support_len) / max(theta22, 1e-12)\n",
    "    return (n ** (-1.0/5.0)) * (val ** (1.0/5.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249b3cba",
   "metadata": {},
   "source": [
    "## Experiments & plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55129dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(\n",
    "    alpha_list=(2.0, 2.0),\n",
    "    beta_list=(2.0, 5.0),\n",
    "    n_list=(200, 400, 800, 1600),\n",
    "    N_list=(1, 2, 3, 4, 5),\n",
    "    sigma=1.0,\n",
    "    seed=517,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Sweep over n, N, and (alpha,beta) and plot h_AMISE_hat.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # h_AMISE vs n (fix alpha,beta,N)\n",
    "    a_fix, b_fix, N_fix = 2.0, 2.0, 3\n",
    "    h_vs_n = []\n",
    "    for n in n_list:\n",
    "        X, Y = simulate(n, a_fix, b_fix, sigma, rng)\n",
    "        th22 = theta22_hat(X, Y, N_fix)\n",
    "        s2 = sigma2_hat(X, Y, N_fix)\n",
    "        h_vs_n.append(h_AMISE_hat(n, s2, th22, 1.0))\n",
    "    h_vs_n = np.array(h_vs_n)\n",
    "\n",
    "    # h_AMISE vs N (fix alpha,beta,n)\n",
    "    n_fix = 800\n",
    "    Xb, Yb = simulate(n_fix, a_fix, b_fix, sigma, rng)\n",
    "    h_vs_N = []\n",
    "    valid_N = [N for N in N_list if 5 * N < n_fix]\n",
    "    for N in valid_N:\n",
    "        th22 = theta22_hat(Xb, Yb, N)\n",
    "        s2 = sigma2_hat(Xb, Yb, N)\n",
    "        h_vs_N.append(h_AMISE_hat(n_fix, s2, th22, 1.0))\n",
    "    h_vs_N = np.array(h_vs_N)\n",
    "\n",
    "    # h_AMISE vs Beta shape (vary alpha,beta), fix n, N\n",
    "    n_fix2, N_fix2 = 800, 3\n",
    "    pairs = list(zip(alpha_list, beta_list))\n",
    "    h_vs_shape = []\n",
    "    labels = []\n",
    "    for (a, b) in pairs:\n",
    "        Xs, Ys = simulate(n_fix2, a, b, sigma, rng)\n",
    "        th22 = theta22_hat(Xs, Ys, N_fix2)\n",
    "        s2 = sigma2_hat(Xs, Ys, N_fix2)\n",
    "        h_vs_shape.append(h_AMISE_hat(n_fix2, s2, th22, 1.0))\n",
    "        labels.append(f\"α={a:g}, β={b:g}\")\n",
    "    h_vs_shape = np.array(h_vs_shape)\n",
    "\n",
    "    # Ploting\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(12, 3.8), dpi=130)\n",
    "\n",
    "    ax[0].plot(n_list, h_vs_n, marker=\"o\")\n",
    "    ax[0].set_title(r\"$\\hat h_{\\mathrm{AMISE}}$ vs sample size $n$\")\n",
    "    ax[0].set_xlabel(\"n\"); ax[0].set_ylabel(r\"$\\hat h_{\\mathrm{AMISE}}$\"); ax[0].grid(alpha=.3)\n",
    "\n",
    "    ax[1].plot(valid_N, h_vs_N, marker=\"o\")\n",
    "    ax[1].set_title(r\"$\\hat h_{\\mathrm{AMISE}}$ vs block size $N$ (n=800)\")\n",
    "    ax[1].set_xlabel(\"N\"); ax[1].grid(alpha=.3)\n",
    "\n",
    "    ax[2].bar(range(len(h_vs_shape)), h_vs_shape)\n",
    "    ax[2].set_xticks(range(len(labels)), labels, rotation=20)\n",
    "    ax[2].set_title(r\"$\\hat h_{\\mathrm{AMISE}}$ vs Beta$(\\alpha,\\beta)$ shape (n=800, N=3)\")\n",
    "    ax[2].grid(axis=\"y\", alpha=.3)\n",
    "\n",
    "    fig.suptitle(\"Global bandwidth selection (AMISE plug-in, quartic kernel, homoscedastic)\", y=1.02, fontsize=11)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    \n",
    "    #plt.savefig(\"global_bandwidth_selection.png\", bbox_inches=\"tight\")\n",
    "    #plt.close()\n",
    "    #print(\"Saved figure as 'global_bandwidth_selection.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ba8dd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved figure as 'global_bandwidth_selection.png'\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run_experiment(\n",
    "        alpha_list=(2.0, 2.0, 5.0, 1.5),\n",
    "        beta_list =(2.0, 5.0, 2.0, 4.0),\n",
    "        n_list=(200, 400, 800, 1600, 3200),\n",
    "        N_list=(1, 2, 3, 4, 5, 6),\n",
    "        sigma=1.0,\n",
    "        seed=517,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
